{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory questions"
      ],
      "metadata": {
        "id": "oqxUfVW8fxcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Can we use Bagging for regression problems?**  \n",
        "Yes, Bagging can be used for regression problems. In this case, it combines the predictions of multiple regression models (like Decision Tree Regressors) by averaging their outputs to reduce variance and improve accuracy.\n",
        "\n",
        "**2. What is the difference between multiple model training and single model training?**  \n",
        "Single model training involves building one predictive model, while multiple model training involves training several models independently (e.g., in Bagging) or sequentially (e.g., in Boosting) to form a stronger overall predictor.\n",
        "\n",
        "**3. Explain the concept of feature randomness in Random Forest.**  \n",
        "In Random Forest, feature randomness means that each decision tree considers only a random subset of features when splitting nodes. This helps in decorrelating the trees and improving ensemble diversity.\n",
        "\n",
        "**4. What is OOB (Out-of-Bag) Score?**  \n",
        "The OOB score is an internal validation score in Bagging. Since each model is trained on a bootstrap sample, the data not included (out-of-bag) is used to evaluate model performance, providing an unbiased accuracy estimate.\n",
        "\n",
        "**5. How can you measure the importance of features in a Random Forest model?**  \n",
        "Feature importance can be measured using metrics like Gini Importance or Mean Decrease in Impurity (MDI), which calculates how much each feature contributes to reducing impurity across all trees.\n",
        "\n",
        "**6. Explain the working principle of a Bagging Classifier.**  \n",
        "A Bagging Classifier creates multiple models by training them on different bootstrap samples of the dataset. Their outputs are combined using majority voting for classification tasks to produce the final result.\n",
        "\n",
        "**7. How do you evaluate a Bagging Classifier’s performance?**  \n",
        "You can evaluate it using accuracy, precision, recall, F1-score, confusion matrix, ROC-AUC, or OOB score, depending on the nature of the classification problem.\n",
        "\n",
        "**8. How does a Bagging Regressor work?**  \n",
        "A Bagging Regressor trains multiple regressors on random subsets (with replacement) of the training data and combines their predictions by averaging the outputs.\n",
        "\n",
        "**9. What is the main advantage of ensemble techniques?**  \n",
        "The main advantage is improved performance—higher accuracy and robustness—by reducing variance (Bagging), bias (Boosting), or both.\n",
        "\n",
        "**10. What is the main challenge of ensemble methods?**  \n",
        "The main challenges are increased computational cost, reduced interpretability, and the risk of overfitting if not properly managed.\n",
        "\n",
        "**11. Explain the key idea behind ensemble techniques.**  \n",
        "Ensemble techniques combine the outputs of multiple models to produce a more accurate and stable prediction than any individual model.\n",
        "\n",
        "**12. What is a Random Forest Classifier?**  \n",
        "A Random Forest Classifier is an ensemble of decision trees, where each tree is trained on a random subset of the data and features, and classification is done using majority voting.\n",
        "\n",
        "**13. What are the main types of ensemble techniques?**  \n",
        "The main types are Bagging (Bootstrap Aggregating), Boosting, Stacking, and Voting.\n",
        "\n",
        "**14. What is ensemble learning in machine learning?**  \n",
        "Ensemble learning is a method that combines multiple models to improve predictive performance and generalization compared to individual models.\n",
        "\n",
        "**15. When should we avoid using ensemble methods?**  \n",
        "Ensemble methods should be avoided when interpretability is crucial, when computational resources are limited, or when a single model performs sufficiently well.\n",
        "\n",
        "**16. How does Bagging help in reducing overfitting?**  \n",
        "Bagging reduces overfitting by training multiple models on different random samples, thereby reducing the variance and making the model more generalizable.\n",
        "\n",
        "**17. Why is Random Forest better than a single Decision Tree?**  \n",
        "Random Forest is better because it reduces overfitting and variance by averaging multiple decision trees trained on different data and feature subsets.\n",
        "\n",
        "**18. What is the role of bootstrap sampling in Bagging?**  \n",
        "Bootstrap sampling creates diverse training subsets by sampling with replacement. This diversity among models helps in reducing overfitting and variance.\n",
        "\n",
        "**19. What are some real-world applications of ensemble techniques?**  \n",
        "Applications include spam detection, fraud detection, image classification, medical diagnosis, stock price prediction, and recommendation systems.\n",
        "\n",
        "**20. What is the difference between Bagging and Boosting?**  \n",
        "Bagging trains models independently in parallel on random subsets, focusing on variance reduction. Boosting trains models sequentially, where each model corrects errors from the previous ones, aiming to reduce both bias and variance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ypWgyiuegqiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical question"
      ],
      "metadata": {
        "id": "o5fUQi5Pg1Kk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqgwDH1Pfoa7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset for classification\n",
        "X_c, y_c = load_breast_cancer(return_X_y=True)\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c, y_c, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset for regression\n",
        "X_r, y_r = load_diabetes(return_X_y=True)\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_r, y_r, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Classifier using Decision Trees\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "model.fit(X_train_c, y_train_c)\n",
        "print(\"Accuracy:\", model.score(X_test_c, y_test_c))\n"
      ],
      "metadata": {
        "id": "uUWKIcpNhfPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Regressor and evaluate using MSE\n",
        "model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "model.fit(X_train_r, y_train_r)\n",
        "preds = model.predict(X_test_r)\n",
        "print(\"MSE:\", mean_squared_error(y_test_r, preds))\n"
      ],
      "metadata": {
        "id": "69q8mdRLhg_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier on Breast Cancer dataset\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_c, y_train_c)\n",
        "importances = rf.feature_importances_\n",
        "print(\"Feature Importances:\", importances)\n"
      ],
      "metadata": {
        "id": "ulGusWufhm2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regressor vs Decision Tree Regressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "tree = DecisionTreeRegressor(random_state=42)\n",
        "rf.fit(X_train_r, y_train_r)\n",
        "tree.fit(X_train_r, y_train_r)\n",
        "print(\"RF MSE:\", mean_squared_error(y_test_r, rf.predict(X_test_r)))\n",
        "print(\"DT MSE:\", mean_squared_error(y_test_r, tree.predict(X_test_r)))\n"
      ],
      "metadata": {
        "id": "1xUt642ZhnhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute OOB Score\n",
        "rf = RandomForestClassifier(oob_score=True, random_state=42)\n",
        "rf.fit(X_train_c, y_train_c)\n",
        "print(\"OOB Score:\", rf.oob_score_)\n"
      ],
      "metadata": {
        "id": "5mlGq8euhpsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging Classifier using SVM\n",
        "svm_bag = BaggingClassifier(base_estimator=SVC(probability=True), n_estimators=10, random_state=42)\n",
        "svm_bag.fit(X_train_c, y_train_c)\n",
        "print(\"Accuracy (SVM Bagging):\", svm_bag.score(X_test_c, y_test_c))\n"
      ],
      "metadata": {
        "id": "2k4WfP9lhtRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RF with different number of trees\n",
        "for n in [10, 50, 100, 200]:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train_c, y_train_c)\n",
        "    print(f\"Trees: {n}, Accuracy: {rf.score(X_test_c, y_test_c)}\")\n"
      ],
      "metadata": {
        "id": "JuALTVg2hwGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging with Logistic Regression\n",
        "log_bag = BaggingClassifier(base_estimator=LogisticRegression(max_iter=1000), n_estimators=10, random_state=42)\n",
        "log_bag.fit(X_train_c, y_train_c)\n",
        "probs = log_bag.predict_proba(X_test_c)[:, 1]\n",
        "print(\"AUC Score:\", roc_auc_score(y_test_c, probs))\n"
      ],
      "metadata": {
        "id": "agAiFgYLhx70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RF Regressor feature importance\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train_r, y_train_r)\n",
        "print(\"Feature Importances:\", rf.feature_importances_)\n"
      ],
      "metadata": {
        "id": "EF0V4kn5hz9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare accuracy: Bagging vs Random Forest\n",
        "bag = BaggingClassifier(n_estimators=50, random_state=42)\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "bag.fit(X_train_c, y_train_c)\n",
        "rf.fit(X_train_c, y_train_c)\n",
        "print(\"Bagging Accuracy:\", bag.score(X_test_c, y_test_c))\n",
        "print(\"RF Accuracy:\", rf.score(X_test_c, y_test_c))\n"
      ],
      "metadata": {
        "id": "Z_8_VJCPh1vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV on RF Classifier\n",
        "param_grid = {'n_estimators': [50, 100], 'max_depth': [3, 5, None]}\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X_train_c, y_train_c)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "VCVldw7Uh3vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging Regressor with different base estimators\n",
        "for base in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    model = BaggingRegressor(base_estimator=base, n_estimators=10, random_state=42)\n",
        "    model.fit(X_train_r, y_train_r)\n",
        "    print(type(base).__name__, \"MSE:\", mean_squared_error(y_test_r, model.predict(X_test_r)))\n"
      ],
      "metadata": {
        "id": "zEVlZNIhh3lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze misclassified samples in RF\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_c, y_train_c)\n",
        "y_pred = rf.predict(X_test_c)\n",
        "misclassified = np.where(y_pred != y_test_c)[0]\n",
        "print(\"Number of misclassified samples:\", len(misclassified))\n"
      ],
      "metadata": {
        "id": "MA6ob9CDh8W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Bagging vs Single Decision Tree\n",
        "bag = BaggingClassifier(random_state=42)\n",
        "tree = DecisionTreeClassifier()\n",
        "bag.fit(X_train_c, y_train_c)\n",
        "tree.fit(X_train_c, y_train_c)\n",
        "print(\"Bagging Accuracy:\", bag.score(X_test_c, y_test_c))\n",
        "print(\"Decision Tree Accuracy:\", tree.score(X_test_c, y_test_c))\n"
      ],
      "metadata": {
        "id": "0lfTD1o6h_52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix of RF\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_c, y_train_c)\n",
        "ConfusionMatrixDisplay.from_estimator(rf, X_test_c, y_test_c)\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V6RNsySciBhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking Classifier with Decision Tree, SVM, and Logistic Regression\n",
        "estimators = [('dt', DecisionTreeClassifier()), ('svm', SVC(probability=True))]\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stack.fit(X_train_c, y_train_c)\n",
        "print(\"Stacking Accuracy:\", stack.score(X_test_c, y_test_c))\n"
      ],
      "metadata": {
        "id": "1dNogTGpiDca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 important features in RF\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_c, y_train_c)\n",
        "importances = pd.Series(rf.feature_importances_).sort_values(ascending=False)\n",
        "print(\"Top 5 important features:\\n\", importances.head())\n"
      ],
      "metadata": {
        "id": "z-XgtDFXiFgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging Classifier Precision, Recall, F1\n",
        "model = BaggingClassifier(random_state=42)\n",
        "model.fit(X_train_c, y_train_c)\n",
        "y_pred = model.predict(X_test_c)\n",
        "print(classification_report(y_test_c, y_pred))\n"
      ],
      "metadata": {
        "id": "ISy-gDGCiHo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Effect of max_depth in RF\n",
        "for d in [2, 5, 10, None]:\n",
        "    rf = RandomForestClassifier(max_depth=d, random_state=42)\n",
        "    rf.fit(X_train_c, y_train_c)\n",
        "    print(f\"Max Depth: {d}, Accuracy: {rf.score(X_test_c, y_test_c)}\")\n"
      ],
      "metadata": {
        "id": "FagmP7ijiJPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC-AUC Score of RF\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_c, y_train_c)\n",
        "probs = rf.predict_proba(X_test_c)[:, 1]\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test_c, probs))\n"
      ],
      "metadata": {
        "id": "AKWxn4WviLOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Bagging with cross-validation\n",
        "bag = BaggingClassifier(random_state=42)\n",
        "scores = cross_val_score(bag, X_c, y_c, cv=5)\n",
        "print(\"Cross-Val Scores:\", scores)\n",
        "print(\"Mean Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "H3EJmH1KiNdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision-Recall Curve for RF\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_c, y_train_c)\n",
        "probs = rf.predict_proba(X_test_c)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test_c, probs)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MLDZ5IfeiPJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking Classifier with RF and Logistic Regression\n",
        "estimators = [('rf', RandomForestClassifier()), ('lr', LogisticRegression(max_iter=1000))]\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stack.fit(X_train_c, y_train_c)\n",
        "print(\"Stacked Accuracy:\", stack.score(X_test_c, y_test_c))\n"
      ],
      "metadata": {
        "id": "FN9k_70HiQmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging Regressor with different bootstrap sizes\n",
        "for bs in [0.5, 0.7, 1.0]:\n",
        "    model = BaggingRegressor(max_samples=bs, random_state=42)\n",
        "    model.fit(X_train_r, y_train_r)\n",
        "    mse = mean_squared_error(y_test_r, model.predict(X_test_r))\n",
        "    print(f\"Bootstrap: {bs}, MSE: {mse}\")\n"
      ],
      "metadata": {
        "id": "S53_jmh2iSLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}